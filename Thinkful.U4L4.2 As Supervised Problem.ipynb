{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised NLP requires a pre-labelled dataset for training and testing, and is generally interested in categorizing text in various ways. In this case, we are going to try to predict whether a sentence comes from _Alice in Wonderland_ by Lewis Carroll or _Persuasion_ by Jane Austen. We can use any of the supervised models we've covered previously, as long as they allow categorical outcomes. In this case, we'll try Random Forests, SVM, and KNN.\n",
    "\n",
    "Our feature-generation approach will be something called _BoW_, or _Bag of Words_. BoW is quite simple: For each sentence, we count how many times each word appears. We will then use those counts as features.\n",
    "\n",
    "**Note**: Since processing all the text takes around ~5-10 minutes, in the cell below we are taking only the first tenth of each text. If you want to experiment, feel free to change the following code in the next cell:\n",
    "\n",
    "```python\n",
    "alice = text_cleaner(alice[:int(len(alice)/10)])\n",
    "persuasion = text_cleaner(persuasion[:int(len(persuasion)/10)])\n",
    "```\n",
    "to \n",
    "\n",
    "```python\n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not \n",
    "    # recognize: the double dash '--'. Better get rid of it now!\n",
    "    text = re.sub(r'--', ' ', text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic.\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "\n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                         (I, shall, be, late, !, ')  Carroll"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to bag some words!  Since spaCy has already tokenized and labelled our data, we can move directly to recording how often various words occur.  We will exclude stopwords and punctuation.  In addition, in an attempt to keep our feature space from exploding, we will work with lemmas (root words) rather than the raw text terms, and we'll only use the 2000 most common words for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row{}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row0\n",
      "Processing row50\n",
      "Processing row100\n",
      "Processing row150\n",
      "Processing row200\n",
      "Processing row250\n",
      "Processing row300\n",
      "Processing row350\n",
      "Processing row400\n",
      "Processing row450\n",
      "Processing row500\n",
      "Processing row550\n",
      "Processing row600\n",
      "Processing row650\n",
      "Processing row700\n",
      "Processing row750\n",
      "Processing row800\n",
      "Processing row850\n",
      "Processing row900\n",
      "Processing row950\n",
      "Processing row1000\n",
      "Processing row1050\n",
      "Processing row1100\n",
      "Processing row1150\n",
      "Processing row1200\n",
      "Processing row1250\n",
      "Processing row1300\n",
      "Processing row1350\n",
      "Processing row1400\n",
      "Processing row1450\n",
      "Processing row1500\n",
      "Processing row1550\n",
      "Processing row1600\n",
      "Processing row1650\n",
      "Processing row1700\n",
      "Processing row1750\n",
      "Processing row1800\n",
      "Processing row1850\n",
      "Processing row1900\n",
      "Processing row1950\n",
      "Processing row2000\n",
      "Processing row2050\n",
      "Processing row2100\n",
      "Processing row2150\n",
      "Processing row2200\n",
      "Processing row2250\n",
      "Processing row2300\n",
      "Processing row2350\n",
      "Processing row2400\n",
      "Processing row2450\n",
      "Processing row2500\n",
      "Processing row2550\n",
      "Processing row2600\n",
      "Processing row2650\n",
      "Processing row2700\n",
      "Processing row2750\n",
      "Processing row2800\n",
      "Processing row2850\n",
      "Processing row2900\n",
      "Processing row2950\n",
      "Processing row3000\n",
      "Processing row3050\n",
      "Processing row3100\n",
      "Processing row3150\n",
      "Processing row3200\n",
      "Processing row3250\n",
      "Processing row3300\n",
      "Processing row3350\n",
      "Processing row3400\n",
      "Processing row3450\n",
      "Processing row3500\n",
      "Processing row3550\n",
      "Processing row3600\n",
      "Processing row3650\n",
      "Processing row3700\n",
      "Processing row3750\n",
      "Processing row3800\n",
      "Processing row3850\n",
      "Processing row3900\n",
      "Processing row3950\n",
      "Processing row4000\n",
      "Processing row4050\n",
      "Processing row4100\n",
      "Processing row4150\n",
      "Processing row4200\n",
      "Processing row4250\n",
      "Processing row4300\n",
      "Processing row4350\n",
      "Processing row4400\n",
      "Processing row4450\n",
      "Processing row4500\n",
      "Processing row4550\n",
      "Processing row4600\n",
      "Processing row4650\n",
      "Processing row4700\n",
      "Processing row4750\n",
      "Processing row4800\n",
      "Processing row4850\n",
      "Processing row4900\n",
      "Processing row4950\n",
      "Processing row5000\n",
      "Processing row5050\n",
      "Processing row5100\n",
      "Processing row5150\n",
      "Processing row5200\n",
      "Processing row5250\n",
      "Processing row5300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>walter</th>\n",
       "      <th>with</th>\n",
       "      <th>orange</th>\n",
       "      <th>plainly</th>\n",
       "      <th>carteret</th>\n",
       "      <th>shin</th>\n",
       "      <th>cur</th>\n",
       "      <th>estate</th>\n",
       "      <th>poetry</th>\n",
       "      <th>...</th>\n",
       "      <th>force</th>\n",
       "      <th>claws</th>\n",
       "      <th>care</th>\n",
       "      <th>rejoice</th>\n",
       "      <th>laughing</th>\n",
       "      <th>interesting</th>\n",
       "      <th>sir</th>\n",
       "      <th>roughly</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3064 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  size walter with orange plainly carteret shin cur estate poetry  ... force  \\\n",
       "0    0      0    0      0       0        0    0   0      0      0  ...     0   \n",
       "1    0      0    0      0       0        0    0   0      0      0  ...     0   \n",
       "2    0      0    0      0       0        0    0   0      0      0  ...     0   \n",
       "3    0      0    0      0       0        0    0   0      0      0  ...     0   \n",
       "4    0      0    0      0       0        0    0   0      0      0  ...     0   \n",
       "\n",
       "  claws care rejoice laughing interesting sir roughly  \\\n",
       "0     0    0       0        0           0   0       0   \n",
       "1     0    0       0        0           0   0       0   \n",
       "2     0    0       0        0           0   0       0   \n",
       "3     0    0       0        0           0   0       0   \n",
       "4     0    0       0        0           0   0       0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "3                                      (Oh, dear, !)     Carroll  \n",
       "4                         (I, shall, be, late, !, ')     Carroll  \n",
       "\n",
       "[5 rows x 3064 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out BoW\n",
    "\n",
    "Now let's give the bag of words features a whirl by trying a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9871473354231975\n",
      "\n",
      "Test set score: 0.8923872180451128\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence', 'text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy overfitting, Batman! Overfitting is a known problem when using bag of words, since it basically involves throwing a massive number of features at a model – some of those features (in this case, word frequencies) will capture noise in the training set. Since overfitting is also a known problem with Random Forests, the divergence between training score and test score is expected.\n",
    "\n",
    "\n",
    "## BoW with Logistic Regression\n",
    "\n",
    "Let's try a technique with some protection against overfitting due to extraneous features – logistic regression with ridge regularization (from ridge regression, also called L2 regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3190, 3062) (3190,)\n",
      "Training set score: 0.9579937304075236\n",
      "\n",
      "Test set score: 0.9168233082706767\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', solver='lbfgs') # No need to specify l2 as it's the default. But we put it for demonstration.\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression performs a bit better than the random forest.  \n",
    "\n",
    "# BoW with Gradient Boosting\n",
    "\n",
    "And finally, let's see what gradient boosting can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8846394984326019\n",
      "\n",
      "Test set score: 0.8731203007518797\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like logistic regression is the winner, but there's room for improvement.\n",
    "\n",
    "# Same model, new inputs\n",
    "\n",
    "What if we feed the model a different novel by Jane Austen, like _Emma_?  Will it be able to distinguish Austen from Carroll with the same level of accuracy if we insert a different sample of Austen's writing?\n",
    "\n",
    "First, we need to process _Emma_ the same way we processed the other data, and combine it with the Alice data. Remember that for computation time concerns, we only took the first tenth of the Alice text. Emma is pretty long. **So in order to get comparable length texts, we take the first sixtieth of Emma**. Again, if you want to experiment, you can take the whole texts of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n"
     ]
    }
   ],
   "source": [
    "# Clean the Emma data.\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "emma = re.sub(r'VOLUME \\w+', '', emma)\n",
    "emma = re.sub(r'CHAPTER \\w+', '', emma)\n",
    "emma = text_cleaner(emma)\n",
    "print(emma[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse our cleaned data.\n",
    "emma_doc = nlp(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row0\n",
      "Processing row50\n",
      "Processing row100\n",
      "Processing row150\n",
      "Processing row200\n",
      "Processing row250\n",
      "Processing row300\n",
      "Processing row350\n",
      "Processing row400\n",
      "Processing row450\n",
      "Processing row500\n",
      "Processing row550\n",
      "Processing row600\n",
      "Processing row650\n",
      "Processing row700\n",
      "Processing row750\n",
      "Processing row800\n",
      "Processing row850\n",
      "Processing row900\n",
      "Processing row950\n",
      "Processing row1000\n",
      "Processing row1050\n",
      "Processing row1100\n",
      "Processing row1150\n",
      "Processing row1200\n",
      "Processing row1250\n",
      "Processing row1300\n",
      "Processing row1350\n",
      "Processing row1400\n",
      "Processing row1450\n",
      "Processing row1500\n",
      "Processing row1550\n",
      "Processing row1600\n",
      "Processing row1650\n",
      "Processing row1700\n",
      "Processing row1750\n",
      "Processing row1800\n",
      "Processing row1850\n",
      "Processing row1900\n",
      "Processing row1950\n",
      "Processing row2000\n",
      "Processing row2050\n",
      "Processing row2100\n",
      "Processing row2150\n",
      "Processing row2200\n",
      "Processing row2250\n",
      "Processing row2300\n",
      "Processing row2350\n",
      "Processing row2400\n",
      "Processing row2450\n",
      "Processing row2500\n",
      "Processing row2550\n",
      "Processing row2600\n",
      "Processing row2650\n",
      "Processing row2700\n",
      "Processing row2750\n",
      "Processing row2800\n",
      "Processing row2850\n",
      "Processing row2900\n",
      "Processing row2950\n",
      "Processing row3000\n",
      "Processing row3050\n",
      "Processing row3100\n",
      "Processing row3150\n",
      "Processing row3200\n",
      "Processing row3250\n",
      "Processing row3300\n",
      "Processing row3350\n",
      "Processing row3400\n",
      "Processing row3450\n",
      "Processing row3500\n",
      "Processing row3550\n",
      "Processing row3600\n",
      "Processing row3650\n",
      "Processing row3700\n",
      "Processing row3750\n",
      "Processing row3800\n",
      "Processing row3850\n",
      "Processing row3900\n",
      "Processing row3950\n",
      "Processing row4000\n",
      "Processing row4050\n",
      "Processing row4100\n",
      "Processing row4150\n",
      "Processing row4200\n",
      "Processing row4250\n",
      "Processing row4300\n",
      "Processing row4350\n",
      "Processing row4400\n",
      "Processing row4450\n",
      "Processing row4500\n",
      "Processing row4550\n",
      "Processing row4600\n",
      "Processing row4650\n",
      "Processing row4700\n",
      "Processing row4750\n",
      "Processing row4800\n",
      "Processing row4850\n",
      "Processing row4900\n",
      "Processing row4950\n",
      "Processing row5000\n",
      "Processing row5050\n",
      "Processing row5100\n",
      "Processing row5150\n",
      "Processing row5200\n",
      "Processing row5250\n",
      "Processing row5300\n",
      "Processing row5350\n",
      "Processing row5400\n",
      "Processing row5450\n",
      "Processing row5500\n",
      "Processing row5550\n",
      "Processing row5600\n",
      "Processing row5650\n",
      "Processing row5700\n",
      "Processing row5750\n",
      "Processing row5800\n",
      "Processing row5850\n",
      "Processing row5900\n",
      "Processing row5950\n",
      "Processing row6000\n",
      "Processing row6050\n",
      "Processing row6100\n",
      "Processing row6150\n",
      "Processing row6200\n",
      "Processing row6250\n",
      "Processing row6300\n",
      "Processing row6350\n",
      "Processing row6400\n",
      "Processing row6450\n",
      "Processing row6500\n",
      "Processing row6550\n",
      "Processing row6600\n",
      "Processing row6650\n",
      "Processing row6700\n",
      "Processing row6750\n",
      "Processing row6800\n",
      "Processing row6850\n",
      "Processing row6900\n",
      "Processing row6950\n",
      "Processing row7000\n",
      "Processing row7050\n",
      "Processing row7100\n",
      "Processing row7150\n",
      "Processing row7200\n",
      "Processing row7250\n",
      "Processing row7300\n",
      "Processing row7350\n",
      "Processing row7400\n",
      "Processing row7450\n",
      "Processing row7500\n",
      "Processing row7550\n",
      "Processing row7600\n",
      "Processing row7650\n",
      "Processing row7700\n",
      "Processing row7750\n",
      "Processing row7800\n",
      "Processing row7850\n",
      "Processing row7900\n",
      "Processing row7950\n",
      "Processing row8000\n",
      "Processing row8050\n",
      "Processing row8100\n",
      "Processing row8150\n",
      "Processing row8200\n",
      "Processing row8250\n",
      "Processing row8300\n",
      "Processing row8350\n",
      "Processing row8400\n",
      "Processing row8450\n",
      "Processing row8500\n",
      "Processing row8550\n",
      "Processing row8600\n",
      "Processing row8650\n",
      "Processing row8700\n",
      "Processing row8750\n",
      "Processing row8800\n",
      "Processing row8850\n",
      "Processing row8900\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Build a new Bag of Words data frame for Emma word counts.\n",
    "# We'll use the same common words from Alice and Persuasion.\n",
    "emma_sentences = pd.DataFrame(emma_sents)\n",
    "emma_bow = bow_features(emma_sentences, common_words)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.872554950594878\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>8347</td>\n",
       "      <td>558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>706</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    Austen  Carroll\n",
       "row_0                   \n",
       "Austen     8347      558\n",
       "Carroll     706      307"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can model it!\n",
    "# Let's use logistic regression again.\n",
    "\n",
    "# Combine the Emma sentence data with the Alice data from the test set.\n",
    "X_Emma_test = np.concatenate((\n",
    "    X_train[y_train[y_train=='Carroll'].index],\n",
    "    emma_bow.drop(['text_sentence', 'text_source'], 1)\n",
    "), axis=0)\n",
    "y_Emma_test = pd.concat([y_train[y_train=='Carroll'],\n",
    "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n",
    "\n",
    "# Model.\n",
    "print('\\nTest set score:', lr.score(X_Emma_test, y_Emma_test))\n",
    "lr_Emma_predicted = lr.predict(X_Emma_test)\n",
    "pd.crosstab(y_Emma_test, lr_Emma_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well look at that!  NLP approaches are generally effective on the same type of material as they were trained on. It looks like this model is actually able to differentiate multiple works by Austen from Alice in Wonderland.  Now the question is whether the model is very good at identifying Austen, or very good at identifying Alice in Wonderland, or both...\n",
    "\n",
    "# Challenge 0:\n",
    "\n",
    "Recall that the logistic regression model's best performance on the test set was 93%.  See what you can do to improve performance.  Suggested avenues of investigation include: Other modeling techniques (SVM?), making more features that take advantage of the spaCy information (include grammar, phrases, POS, etc), making sentence-level features (number of words, amount of punctuation), or including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc), and anything else your heart desires.  Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 90%.  \n",
    "\n",
    "# Challenge 1:\n",
    "Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work.  This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it.\n",
    "\n",
    "Record your work for each challenge in a notebook and submit it below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "Training set score: 0.9843260188087775\n",
      "\n",
      "Test set score: 0.8980263157894737\n"
     ]
    }
   ],
   "source": [
    "# First, let's run an SVM model.\n",
    "# Import required packages.\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(X_train, y_train)\n",
    "print(svc)\n",
    "print('Training set score:', svc.score(X_train, y_train))\n",
    "print('\\nTest set score:', svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma: [('I', 3187), ('Mr.', 1152), (\"'s\", 932), ('Emma', 864), ('Mrs.', 699), ('Miss', 595), ('She', 570), ('Harriet', 506), ('said', 484), ('He', 443)]\n",
      "Alice: [('I', 540), ('said', 456), ('Alice', 395), (\"n't\", 216), (\"'s\", 192), ('little', 125), ('The', 103), ('know', 87), ('like', 84), ('went', 83)]\n",
      "Persuasion: [('I', 1121), ('Anne', 497), (\"'s\", 485), ('She', 327), ('Captain', 297), ('Mrs', 291), ('Elliot', 288), ('Mr', 256), ('He', 225), ('Wentworth', 218)]\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate word frequencies.\n",
    "def word_frequencies(text, include_stop=True):\n",
    "    # Build a list of words.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(words)\n",
    "\n",
    "# The most frequent words:\n",
    "emma_freq = word_frequencies(emma_doc, include_stop=False).most_common(10)\n",
    "print('Emma:', emma_freq)\n",
    "alice_freq = word_frequencies(alice_doc, include_stop=False).most_common(10)\n",
    "print('Alice:', alice_freq)\n",
    "persuasion_freq = word_frequencies(persuasion_doc, include_stop=False).most_common(10)\n",
    "print('Persuasion:', persuasion_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique to Alice: {'know', 'Alice', 'went', \"n't\", 'The', 'like', 'little'}\n",
      "Unique to Persuasion: set()\n",
      "Unique to Emma: {'Mr.', 'Emma', 'Miss', 'Harriet', 'Mrs.'}\n"
     ]
    }
   ],
   "source": [
    "# Pull out just the text from our frequency lists.\n",
    "alice_common = [pair[0] for pair in alice_freq]\n",
    "persuasion_common = [pair[0] for pair in persuasion_freq]\n",
    "emma_common = [pair[0] for pair in emma_freq]\n",
    "\n",
    "# Use sets to find the unique values in each top ten.\n",
    "print('Unique to Alice:', set(alice_common) - set(persuasion_common) - set(emma_common))\n",
    "print('Unique to Persuasion:', set(persuasion_common) - set(alice_common) - set(persuasion_common))\n",
    "print('Unique to Emma:', set(emma_common) - set(alice_common) - set(persuasion_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently there are no unique words in Persuasion, so let's run it again with 25 words each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma: [('I', 3187), ('Mr.', 1152), (\"'s\", 932), ('Emma', 864), ('Mrs.', 699), ('Miss', 595), ('She', 570), ('Harriet', 506), ('said', 484), ('He', 443), ('Weston', 438), ('It', 400), ('thing', 399), ('Knightley', 389), ('Elton', 387), ('think', 381), ('The', 358), ('little', 356), ('good', 340), ('know', 337), ('Woodhouse', 314), ('You', 307), ('Jane', 300), ('But', 293), ('time', 273)]\n",
      "Alice: [('I', 540), ('said', 456), ('Alice', 395), (\"n't\", 216), (\"'s\", 192), ('little', 125), ('The', 103), ('know', 87), ('like', 84), ('went', 83), ('thought', 74), ('Queen', 73), ('time', 68), ('And', 67), ('It', 64), ('King', 61), ('began', 58), ('Turtle', 58), (\"'m\", 57), ('way', 56), ('Hatter', 55), ('Mock', 55), ('Gryphon', 55), (\"'ll\", 53), ('You', 51)]\n",
      "Persuasion: [('I', 1121), ('Anne', 497), (\"'s\", 485), ('She', 327), ('Captain', 297), ('Mrs', 291), ('Elliot', 288), ('Mr', 256), ('He', 225), ('Wentworth', 218), ('The', 209), ('Lady', 191), ('good', 181), ('It', 181), ('little', 175), ('said', 173), ('Charles', 166), ('time', 151), ('think', 149), ('Russell', 148), ('Sir', 144), ('Walter', 141), ('Mary', 138), ('man', 133), ('Musgrove', 130)]\n"
     ]
    }
   ],
   "source": [
    "# The most frequent words:\n",
    "emma_freq = word_frequencies(emma_doc, include_stop=False).most_common(25)\n",
    "print('Emma:', emma_freq)\n",
    "alice_freq = word_frequencies(alice_doc, include_stop=False).most_common(25)\n",
    "print('Alice:', alice_freq)\n",
    "persuasion_freq = word_frequencies(persuasion_doc, include_stop=False).most_common(25)\n",
    "print('Persuasion:', persuasion_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique to Alice: {'began', 'Alice', 'went', \"n't\", 'way', \"'ll\", \"'m\", 'Hatter', 'King', 'Gryphon', 'Queen', 'And', 'like', 'thought', 'Mock', 'Turtle'}\n",
      "Unique to Persuasion: set()\n",
      "Unique to Emma: {'Elton', 'thing', 'Woodhouse', 'Mr.', 'Knightley', 'But', 'Weston', 'Emma', 'Miss', 'Harriet', 'Jane', 'Mrs.'}\n"
     ]
    }
   ],
   "source": [
    "# Pull out just the text from our frequency lists.\n",
    "alice_common = [pair[0] for pair in alice_freq]\n",
    "persuasion_common = [pair[0] for pair in persuasion_freq]\n",
    "emma_common = [pair[0] for pair in emma_freq]\n",
    "\n",
    "# Use sets to find the unique values in each top ten.\n",
    "print('Unique to Alice:', set(alice_common) - set(persuasion_common) - set(emma_common))\n",
    "print('Unique to Persuasion:', set(persuasion_common) - set(alice_common) - set(persuasion_common))\n",
    "print('Unique to Emma:', set(emma_common) - set(alice_common) - set(persuasion_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alice: [('-PRON-', 765), ('say', 479), ('alice', 397), ('be', 258), ('not', 232), ('think', 134), ('go', 133), ('little', 127), ('the', 110), ('know', 107)]\n",
      "Persuasion: [('-PRON-', 2243), ('anne', 497), (\"'s\", 466), ('captain', 303), ('elliot', 295), ('mrs', 291), ('good', 289), ('know', 258), ('think', 257), ('mr', 256)]\n",
      "Emma: [('-PRON-', 5467), ('mr.', 1152), (\"'s\", 886), ('emma', 864), ('mrs.', 699), ('think', 660), ('miss', 613), ('say', 565), ('good', 547), ('know', 541)]\n",
      "Unique to Alice: {'be', 'not', 'the', 'alice', 'go', 'little'}\n",
      "Unique to Persuasion: set()\n",
      "Unique to Emma: {'mrs.', 'mr.', 'emma', 'miss'}\n"
     ]
    }
   ],
   "source": [
    "# Utility function to calculate how frequently lemmas appear in the text.\n",
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of lemmas.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(lemmas)\n",
    "\n",
    "# Instantiate our list of most common lemmas.\n",
    "alice_lemma_freq = lemma_frequencies(alice_doc, include_stop=False).most_common(10)\n",
    "persuasion_lemma_freq = lemma_frequencies(persuasion_doc, include_stop=False).most_common(10)\n",
    "emma_lemma_freq = lemma_frequencies(emma_doc, include_stop=False).most_common(10)\n",
    "print('\\nAlice:', alice_lemma_freq)\n",
    "print('Persuasion:', persuasion_lemma_freq)\n",
    "print('Emma:', emma_lemma_freq)\n",
    "\n",
    "# Again, identify the lemmas common to one text but not the others.\n",
    "alice_lemma_common = [pair[0] for pair in alice_lemma_freq]\n",
    "persuasion_lemma_common = [pair[0] for pair in persuasion_lemma_freq] \n",
    "emma_lemma_common = [pair[0] for pair in emma_lemma_freq]\n",
    "\n",
    "print('Unique to Alice:', set(alice_lemma_common) - set(persuasion_lemma_common) - set(emma_lemma_common))\n",
    "print('Unique to Persuasion:', set(persuasion_lemma_common) - set(alice_lemma_common) - set(persuasion_lemma_common))\n",
    "print('Unique to Emma:', set(emma_lemma_common) - set(alice_lemma_common) - set(persuasion_lemma_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice in Wonderland has 1669 sentences.\n",
      "Persuasion has 3649 sentences.\n",
      "Emma has 8905 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many sentences are in each book.\n",
    "sents_alice = list(alice_doc.sents)\n",
    "sents_persuasion = list(persuasion_doc.sents)\n",
    "sents_emma = list(emma_doc.sents)\n",
    "\n",
    "print(\"Alice in Wonderland has {} sentences.\".format(len(sents_alice)))\n",
    "print(\"Persuasion has {} sentences.\".format(len(sents_persuasion)))\n",
    "print(\"Emma has {} sentences.\".format(len(sents_emma)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>walter</th>\n",
       "      <th>with</th>\n",
       "      <th>orange</th>\n",
       "      <th>plainly</th>\n",
       "      <th>carteret</th>\n",
       "      <th>shin</th>\n",
       "      <th>cur</th>\n",
       "      <th>estate</th>\n",
       "      <th>poetry</th>\n",
       "      <th>...</th>\n",
       "      <th>interesting</th>\n",
       "      <th>sir</th>\n",
       "      <th>roughly</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>punct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>67</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3069 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  size walter with orange plainly carteret shin cur estate poetry  ...  \\\n",
       "0    0      0    0      0       0        0    0   0      0      0  ...   \n",
       "1    0      0    0      0       0        0    0   0      0      0  ...   \n",
       "2    0      0    0      0       0        0    0   0      0      0  ...   \n",
       "3    0      0    0      0       0        0    0   0      0      0  ...   \n",
       "4    0      0    0      0       0        0    0   0      0      0  ...   \n",
       "\n",
       "  interesting sir roughly                                      text_sentence  \\\n",
       "0           0   0       0  (Alice, was, beginning, to, get, very, tired, ...   \n",
       "1           0   0       0  (So, she, was, considering, in, her, own, mind...   \n",
       "2           0   0       0  (There, was, nothing, so, VERY, remarkable, in...   \n",
       "3           0   0       0                                      (Oh, dear, !)   \n",
       "4           0   0       0                         (I, shall, be, late, !, ')   \n",
       "\n",
       "  text_source sent_length adv_count verb_count noun_count punct_count  \n",
       "0     Carroll          67         3         13         12          10  \n",
       "1     Carroll          63         7         11          8           7  \n",
       "2     Carroll          33         6          5          2           4  \n",
       "3     Carroll           3         0          0          0           1  \n",
       "4     Carroll           6         0          2          0           2  \n",
       "\n",
       "[5 rows x 3069 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's copy our word_counts data frame as a form of version control.\n",
    "word_counts2 = word_counts\n",
    "\n",
    "# Add a column for the word counts in each sentence to the data frame.\n",
    "word_counts2['sent_length'] = word_counts2.text_sentence.map(lambda x: len(x))\n",
    "\n",
    "# Let's create a count for parts of speech.\n",
    "# Adverbs in each sentence.\n",
    "sentences2 = word_counts2.text_sentence\n",
    "adv_count = []\n",
    "for sent in sentences2:\n",
    "    advs = 0\n",
    "    for token in sent:\n",
    "        if token.pos_ == 'ADV':\n",
    "            advs +=1\n",
    "    adv_count.append(advs)\n",
    "            \n",
    "# Add adverbs column to data frame.\n",
    "word_counts2['adv_count'] = adv_count\n",
    "\n",
    "\n",
    "# Verbs in each sentence.\n",
    "verb_count = []\n",
    "for sent in sentences2:\n",
    "    verb = 0\n",
    "    for token in sent:\n",
    "        if token.pos_ == 'VERB':\n",
    "            verb +=1\n",
    "    verb_count.append(verb)\n",
    "    \n",
    "# Add verbs column to data frame.\n",
    "word_counts2['verb_count'] = verb_count\n",
    "\n",
    "\n",
    "# Nouns in each sentence:\n",
    "noun_count = []\n",
    "for sent in sentences2:\n",
    "    noun = 0\n",
    "    for token in sent:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            noun +=1\n",
    "    noun_count.append(noun)\n",
    "    \n",
    "# Add nouns column to data frame.\n",
    "word_counts2['noun_count'] = noun_count\n",
    "\n",
    "# Punctuation marks in each sentence.\n",
    "punct_count = []\n",
    "for sent in sentences2:\n",
    "    punct = 0\n",
    "    for token in sent:\n",
    "        if token.pos_ == 'PUNCT':\n",
    "            punct +=1\n",
    "    punct_count.append(punct)\n",
    "    \n",
    "# Add punctuation column to data frame.\n",
    "word_counts2['punct_count'] = punct_count\n",
    "\n",
    "word_counts2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3190, 3067) (3190,)\n",
      "Training set score:  0.9626959247648903\n",
      "\n",
      "Test set score:  0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "# Let's set up our model again.\n",
    "y = word_counts2['text_source']\n",
    "X = np.array(word_counts2.drop(['text_sentence', 'text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=5000)\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score: ', lr.score(X_train, y_train))\n",
    "print('\\nTest set score: ', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score:  0.9855799373040752\n",
      "\n",
      "Test set score:  0.9083646616541353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmchenry/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Let's go back and re-try SVM with the new features.\n",
    "\n",
    "svm = LinearSVC()\n",
    "train = svm.fit(X_train, y_train)\n",
    "print('Training set score: ', svm.score(X_train, y_train))\n",
    "print('\\nTest set score: ', svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>1438</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>134</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0        Austen  Carroll\n",
       "text_source                 \n",
       "Austen         1438       34\n",
       "Carroll         134      522"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see this in crosstab.\n",
    "svm_predicted = lr.predict(X_test)\n",
    "pd.crosstab(y_test, svm_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model seems extremely good at knowing when a sentence is written by Austen, but not very good at identifying Carroll. It is only due to class imbalance that our results appear to be so good (if the classes were equally balanced, our model would be much less effective). Let's try Challenge 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw:\n",
      " [Leaves of Grass by Walt Whitman 1855]\n",
      "\n",
      "\n",
      "Come, said my soul,\n",
      "Such verses for my Body let us write, (\n"
     ]
    }
   ],
   "source": [
    "# Let's load another book. Let's see the different vocabulary in Whitman's *Leaves of Grass*.\n",
    "leaves = gutenberg.raw('whitman-leaves.txt')\n",
    "print('\\nRaw:\\n', leaves[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our cleaner function to clean the text.\n",
    "leaves = text_cleaner(leaves[:int(len(leaves)/10)])\n",
    "\n",
    "# Parse the clean data.\n",
    "leaves_doc = nlp(leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Come, said my soul, Such verses for my Body let us write, (for we are one,) That should I after return, Or, long, long hence, in other spheres, There to some group of mates the chants resuming, (Tallying Earth's soil, trees, winds, tumultuous waves,) Ever with pleas'd smile I may keep on, Ever and ever yet the verses owning as, first, I here and now Signing for Soul and Body, set to them my name, Walt Whitman } One's-Self I Sing One's-self I sing, a simple separate person, Yet utter the word Democratic, the word En-Masse. Of physiology from top to toe I sing, Not physiognomy alone nor brain alone is worthy for the Muse, I say the Form complete is worthier far, The Female equally with the Male I sing. Of Life immense in passion, pulse, and power, Cheerful, for freest action form'd under the laws divine, The Modern Man I sing. } As I Ponder'd in Silence As I ponder'd in silence, Returning upon my poems, considering, lingering long, A Phantom arose before me with distrustful aspect, Terri\n"
     ]
    }
   ],
   "source": [
    "# Preview cleaned text.\n",
    "print(leaves[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by sentences.\n",
    "leaves_sents = [[sent, 'Whitman'] for sent in leaves_doc.sents]\n",
    "\n",
    "leaves_sents = leaves_sents[0:len(alice_sents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row0\n",
      "Processing row50\n",
      "Processing row100\n",
      "Processing row150\n",
      "Processing row200\n",
      "Processing row250\n",
      "Processing row300\n",
      "Processing row350\n",
      "Processing row400\n",
      "Processing row450\n",
      "Processing row500\n",
      "Processing row550\n",
      "Processing row600\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Try a new Bag of Words for Leaves word counts.\n",
    "leaves_sentences = pd.DataFrame(leaves_sents)\n",
    "leaves_bow = bow_features(leaves_sentences, common_words)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words function.\n",
    "def bow_counts(bow_df):\n",
    "    counts = []\n",
    "    # Count number of each feature per sentence.\n",
    "    for sent in bow_df.text_sentence:\n",
    "        len_sent = len(sent)\n",
    "        advs = 0\n",
    "        verbs = 0\n",
    "        nouns = 0\n",
    "        punc = 0\n",
    "        for token in sent:\n",
    "            if token.pos_ == 'ADV':\n",
    "                advs += 1\n",
    "            elif token.pos_ == 'VERB':\n",
    "                verbs += 1\n",
    "            elif token.pos_ == 'NOUN':\n",
    "                nouns += 1\n",
    "            elif token.pos_ == 'PUNCT':\n",
    "                punc += 1\n",
    "        # Append counts to list.\n",
    "        counts.append([len_sent, advs, verbs, nouns, punc])\n",
    "    # Combine original bow_features df with counts.\n",
    "    df = pd.concat([bow_df, pd.DataFrame(counts, columns=['sent_length',\n",
    "                                                          'adv_count',\n",
    "                                                          'verb_count',\n",
    "                                                          'noun_count',\n",
    "                                                          'punc_count'])], axis=1)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>walter</th>\n",
       "      <th>with</th>\n",
       "      <th>orange</th>\n",
       "      <th>plainly</th>\n",
       "      <th>carteret</th>\n",
       "      <th>shin</th>\n",
       "      <th>cur</th>\n",
       "      <th>estate</th>\n",
       "      <th>poetry</th>\n",
       "      <th>...</th>\n",
       "      <th>force</th>\n",
       "      <th>claws</th>\n",
       "      <th>care</th>\n",
       "      <th>rejoice</th>\n",
       "      <th>laughing</th>\n",
       "      <th>interesting</th>\n",
       "      <th>sir</th>\n",
       "      <th>roughly</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Come, ,, said, my, soul, ,, Such, verses, for...</td>\n",
       "      <td>Whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(That, should, I, after, return, ,, Or, ,, lon...</td>\n",
       "      <td>Whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Ever, with, pleas'd, smile, I, may, keep, on,...</td>\n",
       "      <td>Whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, here, and, now, Signing, for, Soul, and, B...</td>\n",
       "      <td>Whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(One's)</td>\n",
       "      <td>Whitman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3064 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  size walter with orange plainly carteret shin cur estate poetry  ... force  \\\n",
       "0    0      0    0      0       0        0    0   0      0      0  ...     0   \n",
       "1    0      0    0      0       0        0    0   0      0      0  ...     0   \n",
       "2    0      0    0      0       0        0    0   0      0      0  ...     0   \n",
       "3    0      0    0      0       0        0    0   0      0      0  ...     0   \n",
       "4    0      0    0      0       0        0    0   0      0      0  ...     0   \n",
       "\n",
       "  claws care rejoice laughing interesting sir roughly  \\\n",
       "0     0    0       0        0           0   0       0   \n",
       "1     0    0       0        0           0   0       0   \n",
       "2     0    0       0        0           0   0       0   \n",
       "3     0    0       0        0           0   0       0   \n",
       "4     0    0       0        0           0   0       0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Come, ,, said, my, soul, ,, Such, verses, for...     Whitman  \n",
       "1  (That, should, I, after, return, ,, Or, ,, lon...     Whitman  \n",
       "2  (Ever, with, pleas'd, smile, I, may, keep, on,...     Whitman  \n",
       "3  (I, here, and, now, Signing, for, Soul, and, B...     Whitman  \n",
       "4                                            (One's)     Whitman  \n",
       "\n",
       "[5 rows x 3064 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaves_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "leaves_sents = [[sent, \"Whitman\"] for sent in leaves_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row0\n",
      "Processing row50\n",
      "Processing row100\n",
      "Processing row150\n",
      "Processing row200\n",
      "Processing row250\n",
      "Processing row300\n",
      "Processing row350\n",
      "Processing row400\n",
      "Processing row450\n",
      "Processing row500\n",
      "Processing row550\n",
      "Processing row600\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Build a new Bag of Words data frame for Leaves word counts.\n",
    "# We'll use the same common words from Alice and Persuasion.\n",
    "leaves_sentences = pd.DataFrame(leaves_sents)\n",
    "leaves_bow = bow_features(leaves_sentences, common_words)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3190, 3067) (3190,)\n",
      "Training set score:  0.9626959247648903\n",
      "\n",
      "Test set score:  0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "# Let's set up our model again.\n",
    "y = word_counts2['text_source']\n",
    "X = np.array(word_counts2.drop(['text_sentence', 'text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=5000)\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score: ', lr.score(X_train, y_train))\n",
    "print('\\nTest set score: ', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
